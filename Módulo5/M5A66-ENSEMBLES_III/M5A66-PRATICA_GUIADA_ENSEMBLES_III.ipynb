{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/dh_logo.png\" align=\"right\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Ensembles: Boosting\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"sequential_boosting_init.png\" align=\"center\" width=\"50%\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### Continuamos nossa discussão de técnicas de [`machine learning`](https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5) para classificação. em nosso último encontro falamos sobre métodos de [Ensembles Paralelos](https://medium.com/analytics-vidhya/ensemble-learning-data-science-3b0a3832c479). Nesse encontro, vamos falar sobre métodos de [Ensembles sequenciais](https://www.analyticsvidhya.com/blog/2021/01/exploring-ensemble-learning-in-machine-learning-world/). Lembrando os tipos de Ensembles:\n",
    "\n",
    "\n",
    "1. [`Parallel Ensemble Methods`](https://bdtechtalks.com/2020/11/12/what-is-ensemble-learning/): métodos de conjunto paralelo em que os `base learners` são gerados em paralelo (exemplos são os métodos de [`Bagging`, `Stacking`, `Voting`](https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de)). A motivação básica dos métodos paralelos é explorar a independência entre os `base learners`, pois o erro pode ser reduzido drasticamente pela média.\n",
    "\n",
    "\n",
    "2. [`Sequential Ensemble Methods`](https://www.educba.com/bagging-and-boosting/): métodos de conjunto sequencial em que os `base learners` são gerados sequencialmente (um exemplo é o método de ([`Boosting`](https://towardsdatascience.com/what-is-boosting-in-machine-learning-2244aa196682)). A motivação básica dos métodos seqüenciais é explorar a dependência entre os `base learners`. O desempenho geral pode ser aprimorado pesando exemplos previamente rotulados incorretamente com maior peso.\n",
    "\n",
    "\n",
    "#### Dentro desta classificação temos:\n",
    "\n",
    "\n",
    "1. Parallel Ensemble Methods\n",
    "\n",
    "    - [`Voting Ensemble`](https://stackabuse.com/ensemble-voting-classification-in-python-with-scikit-learn/);\n",
    "    - [`Stacking Ensemble`](https://www.mariofilho.com/como-fazer-stacking-machine-learning/)\n",
    "    - [`Bagging Ensemble`]()\n",
    "\n",
    "2. Sequential Ensemble Methods\n",
    "\n",
    "    - [`Boosting Ensemble`](https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição de Sequential Boosting Ensemble\n",
    "\n",
    "\n",
    "#### O [`Boosting`](https://www.geeksforgeeks.org/boosting-in-machine-learning-boosting-and-adaboost/) é usado para converter [`weak learners`](https://medium.com/@toprak.mhmt/gradient-boosting-and-weak-learners-1f93726b6fbd#:~:text=The%20term%20Weak%20Learner%20refers,technically%2C%20any%20model%20will%20do.) em [`strong learners`](https://www.functionize.com/blog/boosting-ml-create-strong-learners/). \n",
    "\n",
    "- `Weak Learners`: Geralmente têm uma correlação muito fraca com os rótulos verdadeiros da classe;\n",
    "- `Strong Learners`: Têm uma correlação muito alta entre o modelo e os verdadeiros rótulos da classe.\n",
    "\n",
    "#### [`Boosting`](https://www.youtube.com/watch?v=9IyJ4HvubGo&ab_channel=SASSoftware) envolve treinar iterativamente os `weak learners`, cada um tentando corrigir o erro cometido pelo modelo anterior. Isso é obtido treinando um modelo fraco em todos os dados de treinamento e construindo um segundo modelo que visa corrigir os erros cometidos pelo primeiro modelo. Em seguida, construímos um terceiro modelo que tenta corrigir os erros cometidos pelo segundo modelo e assim por diante. Os modelos são adicionados iterativamente até o modelo final corrigir todos os erros cometidos por todos os modelos anteriores.\n",
    "\n",
    "#### Quando os modelos são adicionados em cada estágio, alguns pesos são atribuídos ao modelo, o que está relacionado à precisão do modelo anterior. Após a adição de um classificador fraco, os pesos são reajustados. Os pontos classificados incorretamente recebem pesos mais altos e os pontos classificados corretamente recebem pesos menores. Essa abordagem fará com que o próximo classificador se concentre nos erros cometidos pelo modelo anterior.\n",
    "\n",
    "#### O impulso reduz o erro de generalização, adotando um modelo de alta e baixa variância e reduzindo a tendência em um nível significativo. Lembre-se, [`Bagging`](https://towardsdatascience.com/bagging-on-low-variance-models-38d3c70259db#:~:text=This%20technique%20is%20effective%20on,intuition%20behind%20it%20is%20crucial.) reduz a variação. Semelhante ao *Bagging*, o reforço também permite trabalhar com os modelos de classificação e regressão. Dê uma olhada nos diagramas abaixo para entender intuitivamente como o impulso funciona em cada um dos estágios. O diagrama abaixo mostra os diferentes estágios de um algoritmo de impulso.\n",
    "\n",
    "<img src=\"boosting_process.png\" align=\"center\" width=\"100%\">\n",
    "\n",
    "#### Vamos entender o diagrama acima. Temos um conjunto de dados `D` e a primeira coisa que faremos no estágio $0$ é treinar um modelo em todo o conjunto de dados. O modelo pode ser uma classificação ou uma regressão. Vamos nomear esse modelo como $M_{0}$ e assumir que este modelo está tentando ajustar uma função $h_{0}(x)$. Então, a função de previsão para este modelo é dada por $y_{pred} = h_{0}(x)$. \n",
    "\n",
    "#### O modelo $0$ foi projetado para ter um alto viés. Geralmente, o reforço é aplicado para um modelo de alto viés e baixa variação. $O$ alto viés em um modelo refere-se basicamente a um alto erro de treinamento. O alto viés surge principalmente devido a algumas suposições incorretas feitas na fase de treinamento.\n",
    "\n",
    "#### Agora, após a construção do primeiro modelo, obteremos, no estágio $0$, o erro de previsão para cada ponto de dados criado pelo modelo $M_{0}$. Portanto, o erro na previsão para qualquer rótulo de classe é dado por $y - y_{pred}$. Lembre-se, existem muitas funções de erro por aí - por exemplo, o erro quadrático, o [`hinge loss error`](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23), o erro de perda logística etc. Mas, por simplicidade, vamos nos concentrar no erro de diferença simples para este exemplo.\n",
    "\n",
    "#### No estágio $1$ tentaremos ajustar um modelo $M_{1}$ aos erros produzidos pelo modelo no estágio $0$. Lembre-se, $M_{1}$ não está treinando nos rótulos de classe reais. $M_{1}$ está treinando nos erros que obtivemos no final do estágio $0$. Digamos que obtemos uma função $h_{1}(x)$, que treinou os erros gerados pelo modelo $M_{0}$. Assim, no final do estágio $1$, o modelo final será realmente a soma ponderada das duas funções de previsão anteriores (como mostrado no diagrama). Atribuiremos os pesos $a_{0}$ e $a_{1}$ a $h_{0}(x)$ e $h_{1}(x)$ respectivamente. Portanto, no final do estágio $1$, o modelo se parece com: \n",
    "\n",
    "<center> \n",
    "$$ \n",
    "F_{1}(x) = a_{0} h_{0}(x) + a_{1} h_{1}(x); \n",
    "$$ \n",
    "</center> \n",
    "\n",
    "#### em que $a_{0}$ e $a_{1}$ são pesos atribuídos às funções de previsão. Lembre-se de que os pesos sempre serão maiores para funções com alto erro de classificação. Dessa forma, podemos criar o próximo modelo na sequência para focar mais nos erros cometidos pelo modelo anterior.\n",
    "\n",
    "#### Da mesma forma, o modelo no final do estágio $2$ terá a função: \n",
    "\n",
    "<center> \n",
    "$$ \n",
    "F_{2}(x) = a_{0} h_{0}(x) + a_{1} h_{1}(x) + a_{2} h_{2}(x). \n",
    "$$ \n",
    "</center>\n",
    "\n",
    "#### Assim, no final de todas as etapas, o modelo final que temos é dado pela soma $\\sum_{i = 1}^{N} a_{i} h_{x}(i)$. Assim, intuitivamente, estamos na verdade reduzindo o erro de treinamento, o que significa em outras palavras, na verdade estamos reduzindo o viés de um modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrando no modelo matemático\n",
    "\n",
    "#### Existem muitas publicações sobre o conceito de [`Boosting`](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/). Em 1988, Michael Kearns publicou o [Thoughts on Hypothesis Boosting](https://www.cis.upenn.edu/~mkearns/papers/boostnote.pdf), que provavelmente é o mais antigo da literatura. Sobre os algoritmos, é possível encontrar algumas referências. Considere, por exemplo, [Improving Regressors using Boosting Techniques](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&rep=rep1&type=pdf), de Harris Drucker. Ou a abordagem de [The Boosting Approach to Machine Learning An Overview](https://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf) de Robert Schapire, entre muitos outros. Para ilustrar o uso de reforço no contexto de regressão, considere a seção em [ The boosting: A new idea of building models.](https://sci-hub.se/10.1016/j.chemolab.2009.09.002), de Dong-Sheng Cao.\n",
    "\n",
    "<br>\n",
    "<img src=\"boosting-algo-7.gif\" align=\"center\" width=\"40%\">\n",
    "<br>\n",
    "\n",
    "#### Considerando os dados com os `samples` $(x_{1}, y_1)$, em que $y_{i} = 0,1$ para `samples` positivos e `samples` negativos respectivamente. Inicialização nossos pesos: $ w_{1,i} = \\frac{1}{2m}, \\frac{1}{2l} $, para  $ y_{i} = 0,1$ respectivamente. Em que  $m$ é o número de `samples` positivos e $l$ é o número de `samples` negativos.\n",
    "\n",
    "\n",
    "#### Para cada $t = 1, 2,...,T$ são executados os passos seguintes:\n",
    "\n",
    "- 1. Normalização dos pesos $ w_{t,i} = \\frac{ w_{t,i} }{ \\sum_{j=1}^{n} w_{t,j} } $.\n",
    "\n",
    "- 2. Seleção dos melhores classificadores em relação ao erro ponderado $ \\varepsilon_{t} = min_{f, p, \\theta} \\sum_{i} w_{i} | h(x_{i}, f, p, \\theta) - y_{i} | $, que define $h_{i}(x) = h(x, f_{t}, p_{t}, \\theta_{t})$ onde $f_{t}, p_{t}, y, \\theta_{t} $ são minimizadores de $\\varepsilon_{t}$.\n",
    "\n",
    "- 3. Atualização dos pesos $ w_{t+1, i} = w_{t, i} \\beta_t^{1-e_{i}} $, em que  $e_{i} = 0$ se o `sample` é classificado corretamente, $e_{i} = 1$ se o `sample` é classificado erroneamente, e $ \\beta_{t} = \\frac{\\varepsilon_{t}}{1 - \\varepsilon_{t}} $.\n",
    "\n",
    "- 4. O classificador final será $ C(x) = 1 $, se $ \\sum_{t = 1}^{T} \\alpha_{t}h_{t}(x) \\geq \\frac{1}{2} \\sum_{t = 1}^{T} \\alpha_{t}$. Do contrário $C(x) = 0$, em que $\\alpha_{t} = log\\bigg(\\frac{1}{\\beta_{t}}\\bigg)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstração de Boosting para Classificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosting_0.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "#### 1) Primeiramente, criamos um classificador base. Ele vai conter erros, e precisamos corrigí-los"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosting_1.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "#### 2) Criamos um segundo classificador, mas que opera em cima dos erros do primeiro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosting_2.jpeg\" align=\"left\" width=\"20%\">\n",
    "\n",
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "#### 3) Continuamos esse laço até chegar na performance desejada. O `output` final é dado pela `Weighted Average` dos sub-modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"boosting_3.png\" align=\"left\" width=\"20%\">\n",
    "\n",
    "<br> \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "#### 4) Nosso modelo final é uma combinação de todos os outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmos baseados em boosting:\n",
    "\n",
    "- `Adaboost`\n",
    "\n",
    "- `GBM (Gradient Boosting Machine)`\n",
    "\n",
    "- `XGBoost (Extreme Gradient Boost)`\n",
    "\n",
    "- `LightGBM`\n",
    "\n",
    "- `CatBoost`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmos de Boosting\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "#### O reforço adaptativo ou [`AdaBoost`](https://www.mygreatlearning.com/blog/adaboost-algorithm/) é um dos algoritmos de reforço mais simples. Geralmente, as florestas aleatórias são usadas para modelagem, vários modelos seqüenciais são criados, cada um corrigindo os erros do último modelo. O [`AdaBoost`](https://www.datacamp.com/community/tutorials/adaboost-classifier-python) atribui pesos às observações preditas incorretamente e o modelo subseqüente trabalha para prever esses valores corretamente.\n",
    "\n",
    "\n",
    "    1 - Árvore de tocos (`weak learners`);\n",
    "    2 - As decisões são ponderadas entre os tocos;\n",
    "    \n",
    "$$\n",
    "\\text{Contribuição} = \\frac{1}{2}\\log\\bigg(\\frac{1 - \\text{Erro Total}}{\\text{Erro Total}}\\bigg),\n",
    "$$\n",
    "\n",
    "    em que o Erro Total é a soma dos pesos das amostras classificadas incorretamente.\n",
    "\n",
    "\n",
    "<img src=\"AdaBoost.png\" align=\"center\" width=\"20%\">\n",
    "\n",
    "    3 - Cada novo toco é criado a partir dos erros de classificação do toco anterior.\n",
    "\n",
    "$$\n",
    "\\text{Peso} = \\text{Peso} \\times \\exp^{\\text{Contribuição}},\n",
    "$$    \n",
    "\n",
    "\n",
    "#### Abaixo estão as etapas para executar o algoritmo `AdaBoost`:\n",
    "\n",
    "\n",
    "1. Inicialmente, todas as observações no conjunto de dados recebem pesos iguais. O valor dos pesos será portanto igual a um dividido pelo número de observações do `DataSet`.\n",
    "\n",
    "2. Um modelo é construído em um subconjunto de dados. Pegando como exemplo as `Decision Trees`, será calculado qual o atributo que melhor separa os dados segundo um critério adotado. Um dos critérios adotados para este cálculo é o Indice [`Gini`](https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382), outro é a [entropia](https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf).\n",
    "\n",
    "3. Usando esse modelo, são feitas previsões em todo o conjunto de dados.\n",
    "\n",
    "4. Os erros são calculados comparando as previsões e os valores reais.\n",
    "\n",
    "5. Ao criar o próximo modelo, pesos mais altos são dados aos pontos de dados que foram previstos incorretamente.\n",
    "\n",
    "6. Os pesos podem ser determinados usando o valor do erro. Por exemplo, quanto maior o erro, maior é o peso atribuído à observação.\n",
    "\n",
    "7. Esse processo é repetido até que a função de erro não seja alterada ou que o limite máximo do número de estimadores seja atingido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiper-Parâmetros:\n",
    "\n",
    "#### A biblioteca de métodos de ensemble [`sklearn.ensemble.AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) apresenta os seguintes hiperparâmetros.\n",
    "\n",
    "- `base_estimators` (estimador base)\n",
    "    - Ajuda a especificar o tipo de estimador de base, ou seja, o algoritmo de aprendizado de máquina a ser usado como aluno de base.\n",
    "\n",
    "\n",
    "- `n_estimators` (n estimadores)\n",
    "    - Ele define o número de estimadores de base. \n",
    "    - O valor padrão é $10$, mas você deve manter um valor mais alto para obter melhor desempenho.\n",
    "\n",
    "\n",
    "- `learning_ratefloat` (taxa de Aprendizagem)\n",
    "    - Este parâmetro controla a contribuição dos estimadores na combinação final.\n",
    "    - Há uma troca entre `learning_rate` e `n_estimators`.\n",
    "    \n",
    "    \n",
    "- `max_depth`(profundidade máxima)\n",
    "    - Define a profundidade máxima do estimador individual.\n",
    "    - Ajuste esse parâmetro para obter o melhor desempenho.\n",
    "\n",
    "\n",
    "- `n_jobs` (n tarefas)\n",
    "    - Especifica o número de processadores que está autorizado a usar.\n",
    "    - Defina o valor como $-1$ para o número máximo de processadores permitido.\n",
    "    \n",
    "    \n",
    "- `random_state` (estado aleatório)\n",
    "    - Um valor inteiro para especificar a divisão de dados aleatórios.\n",
    "    - Um valor definido de random_state sempre produzirá os mesmos resultados se fornecido com os mesmos parâmetros e dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting (GBM)\n",
    "\n",
    "\n",
    "#### O [`Gradient Boosting`](https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/) ou GBM é outro algoritmo de aprendizado de máquina de conjunto que funciona para problemas de [regressão](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) e [classificação](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). O GBM usa a técnica de reforço, combinando vários `weak learnenrs` para formar um `strong learner`. Árvores de regressão são usadas como `base learner`, cada árvore subsequente em série é construída com base nos erros calculados pela árvore anterior.\n",
    "\n",
    "#### Usaremos um exemplo simples para entender o algoritmo GBM. Temos que prever a idade de um grupo de pessoas usando os dados abaixo:\n",
    "\n",
    "\n",
    "<img src=\"gbm1.png\" align=\"left\" width=\"40%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    1. Um valor médio é assumido como o valor previsto para todas as observações no conjunto de dados.\n",
    "\n",
    "    2. Os erros são calculados usando esta previsão média e os valores reais.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"gbm2.png\" align=\"right\" width=\"40%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    3. Um modelo de árvore é criado usando os erros calculados acima como variável de destino. Nosso objetivo é encontrar a melhor divisão para minimizar o erro.\n",
    "\n",
    "$$\n",
    "\\text{Residual} = \\text{Observed} - \\text{Predicted}  \n",
    "$$\n",
    "\n",
    "    4. As previsões desse modelo são combinadas com as previsões 1.\n",
    "    \n",
    "$$\n",
    "\\text{Prediction} = \\text{Initial Guess} + \\text{Learning rate} \\sum_{i = 1}^{\\text{Total Árvores}} \\text{Prediction}_{i}  \n",
    "$$    \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"gbm3.png\" align=\"left\" width=\"40%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    5. Este valor calculado acima é a nova previsão.\n",
    "\n",
    "    6. Novos erros são calculados usando esse valor previsto e o valor real.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"gbm4.png\" align=\"right\" width=\"40%\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    7. As etapas 2 a 6 são repetidas até que o número máximo de iterações seja atingido (ou a função de erro não seja alterada).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uma função de perda conhecida é a [`Cross Entropy`](https://towardsdatascience.com/a-visual-guide-to-gradient-boosted-trees-8d9ed578b33)\n",
    "\n",
    "<center> \n",
    "$$ \n",
    "Loss(p_{i}, q_{i}) = \\sum_{i~\\in~\\text{Classes de saída}}p_{i}\\times\\log(q_{i})\n",
    "$$ \n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Em que temos:\n",
    "\n",
    "- p: Rótulos Observados do `target`;\n",
    "\n",
    "- q: Rótulos Previstos do `target`.\n",
    "    \n",
    "<br>\n",
    "\n",
    "<img src=\"LossFunction.png\" align=\"center\" width=\"50%\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "        return - math.log(yHat)\n",
    "    else:\n",
    "        return - math.log(1 - yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy(yHatvalue, yvalue): -0.0\n"
     ]
    }
   ],
   "source": [
    "yHatvalue = 1\n",
    "yvalue = 1\n",
    "print('CrossEntropy(yHatvalue, yvalue):', CrossEntropy(yHatvalue, yvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A previsão do Ensemble após o $i$-ésimo modelo fica:\n",
    "\n",
    "<center> \n",
    "$$ \n",
    "Pred_{i} = Pred_{i-1} + \\eta \\bigg( - \\frac{\\partial Loss(p_{i}, q_{i})}{\\partial Pred_{i-1}} \\bigg)\n",
    "$$ \n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Em que temos:\n",
    "\n",
    "- $Pred_{i}$: Previsão para a árvore $i$;\n",
    "\n",
    "- $Pred_{i}$: Previsão para a árvore $i-1$;\n",
    "\n",
    "- $\\eta$: Taxa de aprendizado;\n",
    "\n",
    "- $\\frac{\\partial Loss(p_{i}, q_{i})}{\\partial Pred_{i-1}}$: Gradiente da função de perda $Loss(p_{i}, q_{i})$ em relação com respeito ao resultado da árvore anterior $i-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "#### O [`XGBoost`](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) (Extreme Gradient Boosting) é uma implementação avançada do algoritmo de impulso de gradiente. O [`XGBoost`](https://machinelearningmastery.com/extreme-gradient-boosting-ensemble-in-python/) provou ser um algoritmo ML altamente eficaz, amplamente utilizado em competições de aprendizado de máquina e hackathons. \n",
    "\n",
    "#### Ele possui alto poder preditivo e é quase $10$ vezes mais rápido que as outras técnicas de impulso de gradiente. Ele também inclui uma variedade de regularização que reduz o excesso de ajuste e melhora o desempenho geral. Por isso, também é conhecido como técnica de \"reforço regularizado\".\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{\\bigg(\\sum_{i~\\in~\\text{Nós}} \\text{Res}_{i} \\bigg)^{2}}{\\text{Número de Resíduos} + \\lambda}\n",
    "$$\n",
    "\n",
    "#### Em que $\\lambda$ é um termo de regularização. O ganho de informação é então canculado para cada camada.\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\text{Similarity Score}_{\\text{Nó esquerda}} + \\text{Similarity Score}_{\\text{Nó direita}} - \\text{Similarity Score}_{\\text{raiz}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Vamos ver como o [`XGBoost`](https://www.youtube.com/watch?v=OtD8wVaFm6E&ab_channel=StatQuestwithJoshStarmer) é comparativamente melhor do que outras técnicas:\n",
    "\n",
    "1. Regularização:\n",
    "\n",
    "    - A implementação padrão do `GBM` não tem [regularização](https://albertum.medium.com/l1-l2-regularization-in-xgboost-regression-7b2db08a59e0) como o `XGBoost`;\n",
    "\n",
    "    - Assim, o `XGBoost` também ajuda a reduzir o excesso de ajustes.\n",
    "\n",
    "2. Processamento paralelo:\n",
    "\n",
    "    - O `XGBoost` implementa o [processamento paralelo](https://medium.com/blablacar/thinking-before-building-xgboost-parallelization-f1a3f37b6e68) e é mais rápido que o `GBM`;\n",
    "\n",
    "    - O `XGBoost` também suporta a implementação no `Hadoop`;\n",
    "    \n",
    "    - O valor de $\\lambda$ é aplicado de modo a reduzir a sensitividade do modelo para cada observação individual, reduzindo `overfiting`. \n",
    "    \n",
    "        - Para $\\lambda \\gt 0 \\rightarrow \\text{(Similarity Score & Gains)}$ são menores.\n",
    "\n",
    "3. Alta flexibilidade:\n",
    "\n",
    "    - O `XGBoost` permite que os usuários definam objetivos de otimização personalizados e critérios de avaliação, adicionando uma nova dimensão ao modelo.\n",
    "\n",
    "    - Tratamento de valores ausentes:\n",
    "\n",
    "    - O `XGBoost` possui uma rotina integrada para lidar com os valores ausentes.\n",
    "\n",
    "4. Poda de árvores:\n",
    "\n",
    "    - O `XGBoost` faz divisões até o `max_depth` (`default`$ = 6 $) especificado e, em seguida, começa a podar ([`Prune`](https://www.capitalone.com/tech/machine-learning/how-to-control-your-xgboost-model/)) a árvore para trás e remove as divisões além das quais não há ganho positivo;\n",
    "\n",
    "    - Para $(\\text{Gain} - \\gamma) \\le 0 \\rightarrow \\text{Galho Podado}$;\n",
    "    \n",
    "    - Para $(\\text{Gain} - \\gamma) \\gt 0 \\rightarrow \\text{Galho Não Podado}$.    \n",
    "\n",
    "\n",
    "5. Validação cruzada incorporada:\n",
    "\n",
    "    - O `XGBoost` permite que o usuário execute uma validação cruzada a cada iteração do processo de otimização e, portanto, é fácil obter o número ideal exato de iterações de otimização em uma única execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A biblioteca de métodos de ensemble [XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/model.html) apresenta os seguintes hiperparâmetros.\n",
    "\n",
    "- `nthread`\n",
    "\n",
    "    - Isso é usado para processamento paralelo e o número de núcleos no sistema deve ser inserido;\n",
    "    \n",
    "    - Se você deseja executar em todos os núcleos, não insira esse valor. O algoritmo irá detectá-lo automaticamente.\n",
    "\n",
    "- `eta`\n",
    "\n",
    "    - Análogo à taxa de aprendizado em `GBM`;\n",
    "    \n",
    "    - Torna o modelo mais robusto diminuindo os pesos em cada etapa.\n",
    "\n",
    "\n",
    "- `min_child_weight`\n",
    "\n",
    "    - Define a soma mínima de pesos de todas as observações necessárias em uma nó children;\n",
    "    \n",
    "    - Usado para controlar o excesso de ajuste. Valores mais altos impedem um modelo de aprender relações que podem ser altamente específicas para a amostra específica selecionada para uma árvore.\n",
    "\n",
    "\n",
    "- `max_depth`\n",
    "\n",
    "    - É usado para definir a profundidade máxima.\n",
    "    \n",
    "    - Uma profundidade maior permitirá que o modelo aprenda relações muito específicas para uma amostra específica.\n",
    "    \n",
    "    \n",
    "- `max_leaf_nodes`\n",
    "\n",
    "     - O número máximo de nós terminais ou folhas em uma árvore;\n",
    "     \n",
    "     - Pode ser definido no lugar de max_depth. Como as árvores binárias são criadas, uma profundidade de 'n' produziria no máximo 2 ^ n folhas.\n",
    "     - Se isso estiver definido, o GBM ignorará max_depth.\n",
    "     \n",
    "     \n",
    "- `gamma`\n",
    "\n",
    "    - Um nó é dividido apenas quando a divisão resultante fornece uma redução positiva na função de perda. O parâmetro gamma especifica a redução de perda mínima necessária para fazer uma divisão.\n",
    "    \n",
    "    - Torna o algoritmo conservador. Os valores podem variar dependendo da função de perda e devem ser ajustados.\n",
    "    \n",
    "- `subsample`\n",
    "\n",
    "    - O mesmo que a subamostra de GBM. Indica a fração de observações a serem amostradas aleatoriamente para cada árvore;\n",
    "    \n",
    "    - Valores mais baixos tornam o algoritmo mais conservador e evitam o ajuste excessivo, mas valores muito pequenos podem levar a um ajuste insuficiente.\n",
    "\n",
    "- `colsample_bytree`\n",
    "\n",
    "    - É semelhante ao max_features no `GBM`;\n",
    "    \n",
    "    - Indica a fração de colunas a serem amostradas aleatoriamente para cada árvore.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM - Light GBM\n",
    "\n",
    "#### Antes de discutir como o [`Light GBM`](https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/) funciona, primeiro vamos entender porque precisamos desse algoritmo quando temos tantos outros, como os que vimos acima. O [`Light GBM`](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/) supera todos os outros algoritmos quando o conjunto de dados é extremamente grande. Comparado aos outros algoritmos, o [`Light GBM`](https://www.kaggle.com/amar09/fare-prediction-stacked-ensemble-xgboost-lgbm) leva menos tempo para executar em um enorme conjunto de dados.\n",
    "\n",
    "#### [`Light GBM`](https://towardsdatascience.com/tagged/lightgbm) é uma estrutura de impulso de gradiente rápida, distribuída e de alto desempenho, baseada no algoritmo da árvore de decisão, usada para classificação, classificação e muitas outras tarefas de aprendizado de máquina.\n",
    "\n",
    "#### O [`Light GBM`](https://towardsdatascience.com/a-quick-guide-to-lightgbm-library-ef5385db8d10#:~:text=LightGBM(LGBM)%20is%20an%20open,with%20incredible%20speed%20and%20accuracy.) é uma estrutura de impulso de gradiente que usa algoritmos baseados em árvore e segue a abordagem em folha, enquanto outros algoritmos trabalham em um padrão de abordagem em nível. As imagens abaixo ajudarão você a entender a diferença de uma maneira melhor.\n",
    "\n",
    "<img src=\"lgbm.png\" align=\"center\" width=\"50%\">\n",
    "\n",
    "#### O crescimento em folha pode causar excesso de ajuste em conjuntos de dados menores, mas isso pode ser evitado usando o parâmetro `max_depth` para aprender.\n",
    "\n",
    "\n",
    "#### Uma vez que é baseado em algoritmos de árvore de decisão, o [LGBM](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/) divide a árvore em termos de folha com o melhor ajuste, enquanto outros algoritmos de reforço dividem a árvore em profundidade ou nível em vez de em folha. Portanto, ao crescer na mesma folha no `Light GBM`, o algoritmo de folha pode reduzir mais perdas do que o algoritmo de nível e, portanto, resulta em uma precisão muito melhor que raramente pode ser alcançada por qualquer um dos algoritmos de aumento existentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A biblioteca de métodos de ensemble [LGBM](https://lightgbm.readthedocs.io/en/latest/) apresenta os seguintes hiperparâmetros.\n",
    "\n",
    "- `num_iterations`\n",
    "\n",
    "    - Ele define o número de iterações de reforço a serem executadas.\n",
    "\n",
    "- `num_leaves`\n",
    "\n",
    "    - Este parâmetro é usado para definir o número de folhas a serem formadas em uma árvore;\n",
    "    \n",
    "    - No caso do Light GBM, como a divisão ocorre em folhas em vez de em profundidade, num_leaves deve ser menor que 2 ^ (max_depth), caso contrário, pode levar ao sobreajuste.\n",
    "\n",
    "- `min_data_in_leaf`\n",
    "\n",
    "    - Um valor muito pequeno pode causar super ajuste;\n",
    "    \n",
    "    - É também um dos parâmetros mais importantes para lidar com o super ajuste.\n",
    "\n",
    "- `max_depth`\n",
    "\n",
    "    - Ele especifica a profundidade ou o nível máximo até o qual uma árvore pode crescer;\n",
    "    \n",
    "    - Um valor muito alto para esse parâmetro pode causar super ajuste.\n",
    "        \n",
    "- `bagging_fraction`\n",
    "\n",
    "    - É usado para especificar a fração de dados a ser usada para cada iteração;\n",
    "    \n",
    "    - Este parâmetro é geralmente usado para acelerar o treinamento.\n",
    "\n",
    "- `max_bin`\n",
    "\n",
    "    - Define o número máximo de posições em que os valores dos recursos serão agrupados;\n",
    "    \n",
    "    - Um valor menor de max_bin pode economizar muito tempo, pois agrupa os valores dos recursos em compartimentos discretos, o que é computacionalmente barato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost\n",
    "\n",
    "\n",
    "#### O tratamento de variáveis categóricas é um processo tedioso, especialmente quando você tem um grande número dessas variáveis. Quando suas variáveis categóricas têm muitos rótulos, ou seja, são altamente cardinais, a execução de uma codificação `one-hot-encoding` aumenta exponencialmente a dimensionalidade e torna-se realmente difícil trabalhar com o conjunto de dados.\n",
    "\n",
    "#### O [`CatBoost`](https://catboost.ai/) pode lidar automaticamente com variáveis categóricas e não requer amplo pré-processamento de dados, como outros algoritmos de aprendizado de máquina.\n",
    "\n",
    "#### O `Catboost` traz duas grandes vantagens:\n",
    "\n",
    "- `Ordered Boosting`: `CatBoost` faz a amostragem de um novo conjunto de dados independentemente em cada etapa do impulso para obter resíduos não deslocados, aplicando o modelo atual a novos exemplos de treinamento.\n",
    "\n",
    "- `Categorical Features`: `CatBoost` usa um método chamado [`Ordered Target Statistics`](https://medium.com/data-from-the-trenches/how-do-gradient-boosting-algorithms-handle-categorical-variables-e56ace858ba2#:~:text=Ordered%20Target%20Statistics%20Explained,each%20category%20of%20the%20feature.) para o gerenciamento de variávis categóricas, em que a quantidade codificada é uma estimativa do valor esperado do alvo em cada categoria do recurso, que é simplesmente o valor alvo médio para amostras da mesma categoria.\n",
    "\n",
    "#### A biblioteca de métodos de ensemble [CatBoost](https://catboost.ai/docs) apresenta os seguintes hiperparâmetros.\n",
    "\n",
    "- `loss_function`\n",
    "\n",
    "    - Define a métrica a ser usada para o treinamento.\n",
    "\n",
    "- `iterations`\n",
    "\n",
    "    - O número máximo de árvores que podem ser construídas;\n",
    "    \n",
    "    - O número final de árvores pode ser menor ou igual a esse número.\n",
    "        \n",
    "- `learning_rate`\n",
    "\n",
    "    - Define a taxa de aprendizado;\n",
    "    \n",
    "    - Usado para reduzir a etapa do gradiente.\n",
    "\n",
    "- `border_count`\n",
    "\n",
    "    - Especifica o número de divisões para recursos numéricos;\n",
    "    \n",
    "    - É semelhante ao parâmetro `max_bin`.\n",
    "        \n",
    "- `depth`\n",
    "\n",
    "    - Define a profundidade das árvores.\n",
    "\n",
    "- `random_seed`\n",
    "\n",
    "    - Este parâmetro é semelhante ao parâmetro 'random_state' que vimos anteriormente;\n",
    "    \n",
    "    - É um valor inteiro para definir a semente aleatória para treinamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Os `Ensembles` podem aumentar exponencialmente o desempenho do seu modelo e às vezes pode ser o fator decisivo entre o primeiro e o segundo lugar em uma competição. Abordamos várias técnicas de `Ensembles` e vimos como essas técnicas são aplicadas em algoritmos de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametrization\n",
    "\n",
    "#### O ajuste é uma técnica que funciona bem para modelos mais simples, como regressão linear, árvores de decisão, etc. Eles têm apenas alguns hiperparâmetros: `learning_rate`,  `no_of_iterations`,  `alpha, lambda` e é fácil saber o que eles significam.\n",
    "\n",
    "#### Mas os GBMs são um mundo diferente:\n",
    "\n",
    "    - Eles têm um grande número de hiperparâmetros.\n",
    "\n",
    "    - Ao contrário da Floresta Aleatória, suas configurações padrão geralmente não são as ideais para o seu problema.\n",
    "\n",
    "\n",
    "#### Portanto, se você deseja usar os GBMs para modelar seus dados, é necessário obter um [entendimento](https://towardsdatascience.com/understanding-gradient-boosting-machines-using-xgboost-and-lightgbm-parameters-3af1f9db9700) de alto nível do que acontece internamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<img src=\"boosting_hyperparameters.png\" align=\"center\" width=\"60%\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando BoostingClassifiers - Caso Prático (Diabetes)\n",
    "\n",
    "#### Vamos comparar o uso de técnicas de [`Gradient Boosting`](https://towardsdatascience.com/predicting-carcinogens-with-logistic-regression-knn-gradient-boosting-and-molecular-e7952294a08c) e com o da regressão logística, tentando classificar o [diabetes](https://www.kaggle.com/saurabh00007/diabetescsv/download) com base no resultado.\n",
    "\n",
    "#### Essencialmente, o conjunto de dados fornece vários recursos usados para prever a variável de resultado (com diabetes = $1$, sem diabetes = $0$). Primeiramente, foi realizada a extração de características com [`extra-trees`](https://www.statworx.com/at/blog/how-to-speed-up-gradient-boosting-by-a-factor-of-two/) para identificar as características mais importantes na previsão da variável de resultado.\n",
    "\n",
    "#### Em seguida, os seguintes modelos foram executados:\n",
    "\n",
    "    - Extração de recursos;\n",
    "    - Regressão logística;\n",
    "    - Classificador de impulso de gradiente;\n",
    "    - Classificador LightGBM;\n",
    "    - Classificador XGBoost;\n",
    "    - Classificador AdaBoost.\n",
    "    \n",
    "#### A extração de recursos está sendo usada para determinar os recursos mais importantes que influenciam a variável de resultado, ou seja, quais recursos têm a correlação mais forte com a incidência de diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"diabetes.csv\", \n",
    "                 delimiter = ','\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acplicamos o método [`sklearn.ensemble.ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), que implementa um metaestimador que ajusta um número de árvores de decisão aleatórias (também conhecidas como árvores extras) em várias subamostras do conjunto de dados e usa a média para melhorar a precisão preditiva e o sobreajuste de controle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 8), (768,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Outcome']\n",
    "x = df.drop(['Outcome'], \n",
    "            axis = 1\n",
    "           )\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usamos o método [`sklearn.ensemble.ExtraTreesClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html), que implementa um metaestimador que ajusta um número de árvores de decisão aleatórias (também conhecidas como árvores extras) em várias subamostras do conjunto de dados e usa a média para melhorar a precisão preditiva e o sobreajuste de controle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier(n_estimators = 10)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11122585 0.24063798 0.10466717 0.08276247 0.07062426 0.13770334\n",
      " 0.11322537 0.13915356]\n"
     ]
    }
   ],
   "source": [
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pd.Series(model.feature_importances_, \n",
    "           index = x.columns\n",
    "          ).nlargest(9).sort_values().plot.barh());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
       "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na extração do recurso, os atributos $1$ (Glucose), $7$ (Age), $5$ (BMI), $6$ (DiabetesPedigreeFunction), $2$ BloodPressure e $0$ (Pregnacies) apresentaram as pontuações mais altas em termos de importância de atributos, e esses serão incluídos nos modelos para prever a variável de resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   ,  33.6  ,   0.627,  50.   ],\n",
       "       [  1.   ,  85.   ,  66.   ,  26.6  ,   0.351,  31.   ],\n",
       "       [  8.   , 183.   ,  64.   ,  23.3  ,   0.672,  32.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   ,  26.2  ,   0.245,  30.   ],\n",
       "       [  1.   , 126.   ,  60.   ,  30.1  ,   0.349,  47.   ],\n",
       "       [  1.   ,  93.   ,  70.   ,  30.4  ,   0.315,  23.   ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = x.iloc[:, 0]\n",
    "x1 = x.iloc[:, 1]\n",
    "x2 = x.iloc[:, 2]\n",
    "x5 = x.iloc[:, 5]\n",
    "x6 = x.iloc[:, 6]\n",
    "x7 = x.iloc[:, 7]\n",
    "\n",
    "xnew = np.column_stack((x0, x1, x2, x5, x6, x7))\n",
    "#xnew\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_val,y_train,y_val = train_test_split(xnew, \n",
    "                                             y, \n",
    "                                             random_state = 0\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portanto, essas variáveis foram definidas como `xnew` em uma pilha de colunas `numpy` e os dados foram particionados em dados de treinamento e validação com `train_test_split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression Vs. BoostingClassifiers\n",
    "\n",
    "#### Depois de selecionar os recursos relevantes e particionar os dados, uma regressão logística pode ser executada em conjunto com vários classificadores de impulso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "+++++++\n",
      "GradientBoostingClassifier(random_state=0)\n",
      "+++++++\n",
      "LGBMClassifier(learning_rate=0.001, num_leaves=65)\n",
      "+++++++\n",
      "[21:25:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.001, max_delta_step=0, max_depth=1,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "+++++++\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
      "                   learning_rate=0.001, n_estimators=100)\n",
      "+++++++\n",
      "*****************ClassifiersDone*****************\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression().fit(x_train, y_train)\n",
    "print(logreg)\n",
    "print('+++++++')\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbrt = GradientBoostingClassifier(random_state = 0).fit(x_train, y_train)\n",
    "print(gbrt)\n",
    "print('+++++++')\n",
    "\n",
    "#  LightGBM Classifier\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(learning_rate = 0.001, \n",
    "                               num_leaves = 65, \n",
    "                               n_estimators = 100\n",
    "                              ).fit(x_train, y_train)                       \n",
    "print(lgb_model)\n",
    "print('+++++++') \n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(learning_rate = 0.001, \n",
    "                              max_depth = 1, \n",
    "                              n_estimators = 100, \n",
    "                              use_label_encoder = False\n",
    "                             ).fit(x_train, y_train)\n",
    "print(xgb_model)\n",
    "print('+++++++')\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), \n",
    "                             n_estimators = 100, \n",
    "                             algorithm = \"SAMME.R\", \n",
    "                             learning_rate = 0.001\n",
    "                            ).fit(x_train, y_train)\n",
    "print(ada_clf)\n",
    "print('+++++++')\n",
    "\n",
    "print('*****************ClassifiersDone*****************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como pode ser observado, o número de `n_estimators` foi definido como $100$ enquanto a taxa de aprendizado foi definida como $0.001$. Muitas são as configurações e [técnicas do impulso de gradiente](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/), mas, neste caso, a taxa de aprendizado (ou parâmetro de encolhimento) é definida abaixo de $0.1$ para um melhor erro de generalização, enquanto o número de `n_estimators` (ou número de árvores) é definido como $100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A seguir calculamos as pontuações de conjunto de treinamento e validação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trainning_Accuracy</th>\n",
       "      <th>Validation_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.762153</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoost</th>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.817708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>0.642361</td>\n",
       "      <td>0.677083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.748264</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.748264</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Trainning_Accuracy  Validation_Accuracy\n",
       "Logistic                 0.762153             0.796875\n",
       "GradientBoost            0.911458             0.817708\n",
       "LightGBM                 0.642361             0.677083\n",
       "XGBoost                  0.748264             0.750000\n",
       "AdaBoost                 0.748264             0.750000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['Logistic', \n",
    "          'GradientBoost', \n",
    "          'LightGBM', \n",
    "          'XGBoost', \n",
    "          'AdaBoost'\n",
    "         ]\n",
    "\n",
    "Trainning_Accuracy = [logreg.score(x_train, y_train), \n",
    "                      gbrt.score(x_train, y_train), \n",
    "                      lgb_model.score(x_train, y_train), \n",
    "                      xgb_model.score(x_train, y_train), \n",
    "                      ada_clf.score(x_train, y_train)\n",
    "                     ]\n",
    "Validation_Accuracy = [logreg.score(x_val, y_val), \n",
    "                       gbrt.score(x_val, y_val), \n",
    "                       lgb_model.score(x_val, y_val), \n",
    "                       xgb_model.score(x_val, y_val), \n",
    "                       ada_clf.score(x_val, y_val)\n",
    "                      ]\n",
    "pd.DataFrame(list(zip(Trainning_Accuracy, \n",
    "                      Validation_Accuracy\n",
    "                     )\n",
    "                 ),\n",
    "             columns = ['Trainning_Accuracy', 'Validation_Accuracy'], \n",
    "             index = models \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observando os resultados acima, duas coisas são evidentes:\n",
    "\n",
    "1. Somente o `GradientBoostingClassifier` gera uma precisão de validação superior à regressão logística - todos os outros modelos de impulso mostram uma precisão de validação um pouco menor.\n",
    "\n",
    "2. Além disso, a precisão da regressão logística no conjunto de treinamento é um pouco menor que a do conjunto de validação, o que implica que o sobreajuste é um problema menor na regressão logística do que nos modelos de impulso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "#### Os modelos de reforço tornaram-se um tipo de \"caixa preta\" e são cada vez mais invocados para maior precisão. No entanto, esses modelos não fornecem necessariamente a melhor precisão em todos os casos, como vimos aqui e a questão do [`Overfitting`](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4) também deve ser considerada.\n",
    "\n",
    "#### Observou-se para o `GradientBoost` que a precisão do treinamento foi significativamente maior que a precisão da validação e isso indica `Overfit`.\n",
    "\n",
    "#### O `Boosting` funciona com a premissa de combinar vários modelos fracos (por exemplo, muitas árvores de decisão) para aumentar a precisão - daí o motivo pelo qual esses modelos são frequentemente chamados de modelos de conjunto. Embora o `Boosting` possa ser vantajoso, dependendo dos dados com os quais estamos trabalhando, eles apresentam um risco de excesso de ajuste e, por padrão, não devem ser invocados sem considerar os dados em questão e se outros modelos podem ser mais adequados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
