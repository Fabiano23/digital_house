{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prática independente - Gradiente Descendente - Solução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O Gradiente Descendente é um algoritmo de otimização de minimização de uma função custo, ou função erro. o algoritmo procura por direções de gradiente negativo.\n",
    "\n",
    "- Encontre um valor inicial para $X_{0}$\n",
    "- Escreva um laço para $i$, com um número ($i = 0, 1, 2, ... $) de variáveis.\n",
    "- Calcule $s_k$ = -$\\nabla f(x_k)$\n",
    "- Escolha a taxa de aprendizado $\\alpha_{k}$ que minimiza $f(x_k+\\alpha_{k} s_{k})$\n",
    "- Faça $x_{k + 1} = x_{k} + \\alpha_k s_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercicio 1 - Gradiente Descendente: Encontre o valor mínimo para a função de custo $f(x) = x^{3} - 2x^{2} + 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defina a função custo e plot seu valor no invervalo `x = [-1.0, 2,5]` e `y = [0.0, 3.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escreva uma função para alcular ponto mínimo $X_{min}$, començando a busca por $X_{0} = 2$. Imprima também o número de passos aplicados ao Gradiente Descendente. Defina um passo de tamanho $0.1$ a precisão como $0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acima você fez uso de uma taxa de aprendizado, ou o passo do algoritmo, constante, o que para um valor baixo pode significar lenta convergência. Estude o método [`scipy.optimize.fmin()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html) e tente otimizar o número de passos que o algoritmo dá para apresentar um resultado. Em outras palavras, otimize a taxa de aprendizagem do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste também outro método de ajuste da taxa de aprendizado, criando uma \"constante\" que varia de acordo com uma constande de redução $d$, como fisto a seguir. \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha_{t+1} = \\frac{\\alpha_{t}}{(1+t\\times d)}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercício 2 - Regressão Linear: Como a temperatura afeta os ruídos dos grilos. No arquivo `SGD_data.txt`, você encontra  uma base de dados de taxas de [chilreio](https://escola.britannica.com.br/artigo/grilo/481074) de grilos em várias temperaturas. Carregue o conjunto, e plote a dispersão entre `Temperatura` em graus Fahrenheit e os `chilreios/s` para o [Allonemobius fasciatus](https://www.cirrusimage.com/orthoptera_striped_cricket/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seu objetivo é encontrar a equação de reta $h_\\theta(x) = \\theta_0 + \\theta_1 x$ que melhor se ajusta aos pontos. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "J(\\theta_0,\\theta_1) = {1 \\over 2m} \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Aqui o Gradiente terá duas dimensões:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Configure as funções $h$, $J$ e o gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregue os dados nas variáveis $X$ e $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute no algoritmo do gradiente Descentdente, com taxa de aprendizado constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para comparação, tome os valores reais para $\\theta_0$ and $\\theta_1$, aplicando o método [`scipy.stats.linregress(`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html). Discuta os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apresente o gráfico da linha desenhada com seus valores $\\theta_{0}$ e $\\theta_{1}$ em relação aos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estude o caso do Gradiente Descendente em Lote ([Batch Gradient Descent](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)) e outras técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercício 3 - Gradiente Descendente Estocástico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como fizemos acima, no Gradiente Descendente em Lote (batch gradient descent), devemos olhar para cada exemplo em todo o conjunto de treinamento em cada etapa (nos casos em que um conjunto de treinamento é usado para o Gradiente Descendente). Isso pode ser bastante lento se o conjunto de treinamento for suficientemente grande. o Gradiente Descendente Estocástico ([Stochastic Gradient Descent](https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31)) atualizamos nossos valores depois de olhar para cada item no conjunto de treinamento, para que possamos começar a fazer progresso imediatamente. \n",
    "\n",
    "#### Teremos:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m (h_\\theta(x_i)-y_i)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0,\\theta_1) = \\frac{1}{m}  \\sum\\limits_{i=1}^m ((h_\\theta(x_i)-y_i) \\cdot x_i)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Em que: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_{0} + \\theta_{1} x\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "#### Seguimos o algoritmo, com $\\alpha$ constante:\n",
    "\n",
    "- Encontre um valor inicial para $X_{0}$\n",
    "- Escreva um laço para $i$, com um número ($i = 0, 1, 2, ... $) de variáveis.\n",
    "- Calcule $s_k$ = -$\\nabla f(x_k)$\n",
    "- Escolha a taxa de aprendizado $\\alpha_{k}$ que minimiza $f(x_k+\\alpha_{k} s_{k})$\n",
    "- Faça $x_{k + 1} = x_{k} + \\alpha_k s_{k}$\n",
    "\n",
    "#### Quando os dados de amostra tinham $15$ pontos de dados como no exemplo acima, o cálculo do gradiente não era muito caro. Mas, para conjuntos de dados muito grandes, esse não seria o caso. Então, em vez disso, consideramos um algoritmo de Gradiente Descendente Estocástico para regressão linear simples, em que $m$ é o tamanho do conjunto de dados:\n",
    "\n",
    "- Embaralhe aleatoriamente o conjunto de dados.\n",
    "- Escrevar um laço para :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " \\theta_{1} \\\\ \n",
    " \\theta_2 \\\\ \n",
    " \\end{bmatrix} = \n",
    " \\begin{bmatrix}\n",
    " \\theta_1 \\\\ \n",
    " \\theta_2 \\\\ \n",
    " \\end{bmatrix} - \n",
    " \\alpha\\begin{bmatrix}\n",
    " 2(h_\\theta(x_i)-y_i) \\\\ \n",
    " 2x_i(h_\\theta(x_i)-y_i) \\\\ \n",
    " \\end{bmatrix}\n",
    " $$\n",
    "\n",
    "#### Normalmente, com a Gradiente Descendente Estocástico, você percorrerá todo o conjunto de dados de $1$ a $10$ vezes, dependendo da velocidade de convergência dos dados e do tamanho do conjunto de dados.\n",
    "\n",
    "#### Com o Gradiente Descendente em lote, devemos percorrer todo o conjunto de dados antes de fazer qualquer progresso. Porém, com esse algoritmo, podemos progredir imediatamente e continuar avançando conforme analisamos o conjunto de dados. Portanto, a Gradiente Descendente Estocástico é freqüentemente preferido ao lidar com grandes conjuntos de dados.\n",
    "\n",
    "#### Ao contrário da Gradiente Descendente, o Gradiente Descendente Estocástica tenderá a oscilar perto de um valor mínimo em vez de se aproximar continuamente. No entanto, pode nunca convergir para o mínimo. Uma maneira de contornar isso é diminuir lentamente o tamanho do passo $\\alpha$, conforme o algoritmo é executado. No entanto, isso é menos comum do que usar um $\\alpha$ fixo.\n",
    "\n",
    "#### Aplique a seguir o Gradiente Descendente Estocástico para uma regressão linear, em que criamos um conjunto de $500.000$ pontos em torno da linha $y = 2x + 17 + \\epsilon $, para valores de $x= [0, 100]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primeiro, embaralhe aleatoriamente o conjunto de dados. Observe que essa etapa não é estritamente necessária, pois os dados já estão em uma ordem aleatória. No entanto, isso obviamente nem sempre é o caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure a função $h$ e a função de custo para verificar como o valor está melhorando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crie e execute o algoritmo de Gradiente Descendente Estocástico. Para observar seu progresso, faça uma medição de custo a cada etapa. A cada $10.000$ etapas, obtenha um custo médio das últimas $10.000$ etapas e, em seguida, anexe isso a uma variável `cost_list`. Percorra a lista inteira $10$ vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuta os valores para $\\theta_0$ e $ \\theta_1$ encontrados. Plote o custo versus o número de iterações. o que você pode dizer sobre o custo ao longo das iterações?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
